{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-27T08:52:47.680035Z","iopub.execute_input":"2021-11-27T08:52:47.680682Z","iopub.status.idle":"2021-11-27T08:52:47.707820Z","shell.execute_reply.started":"2021-11-27T08:52:47.680597Z","shell.execute_reply":"2021-11-27T08:52:47.707015Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -U tensorflow==2.0.0\n\nimport tensorflow as tf\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:52:47.709403Z","iopub.execute_input":"2021-11-27T08:52:47.709714Z","iopub.status.idle":"2021-11-27T08:53:29.484436Z","shell.execute_reply.started":"2021-11-27T08:52:47.709677Z","shell.execute_reply":"2021-11-27T08:53:29.483674Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# import the rare dataset\nrare_ds = pd.read_csv(\"/kaggle/input/rare-kubernetes-anomalies/RARE.csv\", sep=\"|\", header=0)\nrare_ds.head()\noriginal_rare_ds = rare_ds.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:29.485510Z","iopub.execute_input":"2021-11-27T08:53:29.485790Z","iopub.status.idle":"2021-11-27T08:53:46.312502Z","shell.execute_reply.started":"2021-11-27T08:53:29.485746Z","shell.execute_reply":"2021-11-27T08:53:46.311715Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(rare_ds.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.314532Z","iopub.execute_input":"2021-11-27T08:53:46.314736Z","iopub.status.idle":"2021-11-27T08:53:46.319864Z","shell.execute_reply.started":"2021-11-27T08:53:46.314711Z","shell.execute_reply":"2021-11-27T08:53:46.319054Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"rare_ds.dtypes.unique()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.321554Z","iopub.execute_input":"2021-11-27T08:53:46.322216Z","iopub.status.idle":"2021-11-27T08:53:46.338760Z","shell.execute_reply.started":"2021-11-27T08:53:46.322116Z","shell.execute_reply":"2021-11-27T08:53:46.338119Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We can observe that we have a time series dataset, which contains the timestamp as first column, and anomaly as second column. Thee is the isntance field, that contains a descriptive value of the cluster status on that moment.\nThen, there are a set of prometheus metrics, collected for the specific timestamp. The anomaly is the field that we are going to use for training, and the other entries are numerical entries, that are going to influence the model.\nWe have a total of 10.010 rows for training, and a total of 7063 features, that will need to be selected.\nLet's show the different values for the anomaly and instance field:","metadata":{}},{"cell_type":"code","source":"rare_ds.columns =[col.strip() for col in rare_ds.columns]\nprint(rare_ds[\"anomaly\"].unique())\nprint(rare_ds[\"instance\"].unique())","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.341293Z","iopub.execute_input":"2021-11-27T08:53:46.341477Z","iopub.status.idle":"2021-11-27T08:53:46.356083Z","shell.execute_reply.started":"2021-11-27T08:53:46.341454Z","shell.execute_reply":"2021-11-27T08:53:46.355206Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We can see that the anomaly is just a binary field, that we can use to know if there is an anomaly or not. For our case, we can drop the instance field, as it is not relevant for training based on anomalies.","metadata":{}},{"cell_type":"code","source":"rare_ds.drop(labels=['instance'], axis=1, inplace = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.357619Z","iopub.execute_input":"2021-11-27T08:53:46.358242Z","iopub.status.idle":"2021-11-27T08:53:46.532355Z","shell.execute_reply.started":"2021-11-27T08:53:46.358205Z","shell.execute_reply":"2021-11-27T08:53:46.531559Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":" Now we need to proceed with feature selection. We have a total of 7060 metrics and the objective is to select the relevant ones for the project. In order to do it, we could proceed with manual removal first, using the Kubernetes domain knowledge, to discard the non meaningful ones:\n - any information about resource creation is not relevant, because it depends on the workloads that are existing on each cluster\n - kafka topics, logs... are a consequence of other operations in the cluster, they cannot be part of cluster failures reason\n - any information related about pod status, container status.. that have some fixed id needs to be discarded, as it is something that will change depending on the workloads and cluster","metadata":{}},{"cell_type":"code","source":"final_columns = []\nfor column in sorted(rare_ds.columns):\n    if not column.strip().startswith((\"kafka_topic\", \"kafka_log\", \"kube_configmap\", \"kube_namespace\", \"kube_service\", \"kube_secret\", \"kube_pod_status\", \"kube_pod_container_status\", \"scrape_samples\", \"kafka_server_brokertopic\")):\n        final_columns.append(column.strip())","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.533621Z","iopub.execute_input":"2021-11-27T08:53:46.533894Z","iopub.status.idle":"2021-11-27T08:53:46.545685Z","shell.execute_reply.started":"2021-11-27T08:53:46.533859Z","shell.execute_reply":"2021-11-27T08:53:46.544912Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"features_ds = rare_ds[final_columns].copy()\nfeatures_ds.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.548053Z","iopub.execute_input":"2021-11-27T08:53:46.548443Z","iopub.status.idle":"2021-11-27T08:53:46.786492Z","shell.execute_reply.started":"2021-11-27T08:53:46.548405Z","shell.execute_reply":"2021-11-27T08:53:46.785820Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"After manual selection we will use automated methods. First step would be to remove the constant features. Any metric that is having a constant value, with no relevant variance over the dataset, is a metric that is not useful to discriminate a target. We can find those constant values using variance:","metadata":{}},{"cell_type":"code","source":"features_ds.dtypes.unique()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.789603Z","iopub.execute_input":"2021-11-27T08:53:46.789815Z","iopub.status.idle":"2021-11-27T08:53:46.798110Z","shell.execute_reply.started":"2021-11-27T08:53:46.789782Z","shell.execute_reply":"2021-11-27T08:53:46.797266Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# for all columns that are objects, conver to float\nfor column in features_ds.columns:\n    if features_ds[column].dtype == \"object\":\n        features_ds[column] = features_ds[column].astype(float)\nfeatures_ds.dtypes.unique()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:46.799839Z","iopub.execute_input":"2021-11-27T08:53:46.800271Z","iopub.status.idle":"2021-11-27T08:53:47.095589Z","shell.execute_reply.started":"2021-11-27T08:53:46.800234Z","shell.execute_reply":"2021-11-27T08:53:47.094754Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=0.1)\nsel.fit(features_ds)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:47.097072Z","iopub.execute_input":"2021-11-27T08:53:47.097346Z","iopub.status.idle":"2021-11-27T08:53:48.511578Z","shell.execute_reply.started":"2021-11-27T08:53:47.097308Z","shell.execute_reply":"2021-11-27T08:53:48.510865Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# get a list of the constant features. Those can be removed\nprint(\n    len([\n        x for x in features_ds.columns\n        if x not in features_ds.columns[sel.get_support()]\n    ]))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:48.512656Z","iopub.execute_input":"2021-11-27T08:53:48.512924Z","iopub.status.idle":"2021-11-27T08:53:49.014590Z","shell.execute_reply.started":"2021-11-27T08:53:48.512883Z","shell.execute_reply":"2021-11-27T08:53:49.013735Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"to_strip = [x for x in features_ds.columns if x not in features_ds.columns[sel.get_support()]]\nfeatures_ds_strip = features_ds.drop(labels=to_strip, axis=1)\nfeatures_ds_strip.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:49.016107Z","iopub.execute_input":"2021-11-27T08:53:49.016382Z","iopub.status.idle":"2021-11-27T08:53:49.560214Z","shell.execute_reply.started":"2021-11-27T08:53:49.016345Z","shell.execute_reply":"2021-11-27T08:53:49.559354Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"With that technique we have reduced the number of features to 1740. It is still a long number of features, so we need to apply more feature reduction techniques to select the most relevant ones. As we deal with a large amount of features, we will reduce them using Mutual Information technique:","metadata":{}},{"cell_type":"code","source":"X = features_ds_strip\nY = rare_ds[\"anomaly\"].to_frame()\nprint(X.shape)\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:49.561694Z","iopub.execute_input":"2021-11-27T08:53:49.561962Z","iopub.status.idle":"2021-11-27T08:53:49.568125Z","shell.execute_reply.started":"2021-11-27T08:53:49.561925Z","shell.execute_reply":"2021-11-27T08:53:49.567163Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# select with mutual information\nfrom sklearn.feature_selection import SelectPercentile as SP\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nselector = SP(percentile=2) # select features with top 2% MI scores\nselector.fit(X,Y)\nX_4 = selector.transform(X)\nX_train_4,X_test_4,y_train,y_test = tts(\n    X_4,Y\n    ,random_state=0\n    ,stratify=Y\n)\nmodel_4 = DTC().fit(X_train_4,y_train)\nscore_4 = model_4.score(X_test_4,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:49.569075Z","iopub.execute_input":"2021-11-27T08:53:49.569325Z","iopub.status.idle":"2021-11-27T08:53:50.116018Z","shell.execute_reply.started":"2021-11-27T08:53:49.569281Z","shell.execute_reply":"2021-11-27T08:53:50.115306Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(f\"score_4:{score_4}\")\nprint(X_4.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:50.117403Z","iopub.execute_input":"2021-11-27T08:53:50.117657Z","iopub.status.idle":"2021-11-27T08:53:50.123370Z","shell.execute_reply.started":"2021-11-27T08:53:50.117622Z","shell.execute_reply":"2021-11-27T08:53:50.122686Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# get columns, we have reduced to a total of 35\ncolumns = np.asarray(X.columns.values)\nsupport = np.asarray(selector.get_support())\ncolumns_with_support = columns[support]\nprint(columns_with_support)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:50.124435Z","iopub.execute_input":"2021-11-27T08:53:50.124970Z","iopub.status.idle":"2021-11-27T08:53:50.134147Z","shell.execute_reply.started":"2021-11-27T08:53:50.124932Z","shell.execute_reply":"2021-11-27T08:53:50.133318Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Using the Mutual Information method, for the 3% with bigger MI score, returned a total of 36 columns. With a more reasonable number of features, we can now use visualization to discard the highly correlated ones.","metadata":{}},{"cell_type":"code","source":"# with this reduced amount of features, now we can do a correlation map\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\nx = rare_ds[columns_with_support]\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:50.135555Z","iopub.execute_input":"2021-11-27T08:53:50.136036Z","iopub.status.idle":"2021-11-27T08:53:57.394425Z","shell.execute_reply.started":"2021-11-27T08:53:50.135980Z","shell.execute_reply":"2021-11-27T08:53:57.393743Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"common_features = ['java_lang_memorypool_committed_usage_g1_eden_space_0', 'java_lang_operatingsystem_freephysicalmemorysize_0', 'java_lang_operatingsystem_systemloadaverage_0', 'node_entropy_available_bits_0', 'node_filefd_allocated_0',\n                   'node_load1_0', 'node_memory_Active_anon_bytes_0', 'node_memory_Active_file_bytes_0', 'node_memory_Cached_bytes_0', 'node_memory_Committed_AS_bytes_0', 'node_memory_Inactive_bytes_0', 'node_memory_KernelStack_bytes_0',\n                   'node_memory_MemAvailable_bytes_0', 'node_memory_MemFree_bytes_0', 'node_memory_PageTables_bytes_0', 'node_memory_SReclaimable_bytes_0', 'node_memory_SUnreclaim_bytes_0', 'node_memory_Shmem_bytes_0', 'node_nf_conntrack_entries_0',\n                   'node_nf_conntrack_entries_1', 'node_nf_conntrack_entries_2', 'node_sockstat_TCP_alloc_0', 'node_sockstat_sockets_used_0']","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:57.395345Z","iopub.execute_input":"2021-11-27T08:53:57.395554Z","iopub.status.idle":"2021-11-27T08:53:57.402182Z","shell.execute_reply.started":"2021-11-27T08:53:57.395525Z","shell.execute_reply":"2021-11-27T08:53:57.401444Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Once we have reduced the relevant features to a reasonable number, we can proceed to train the model, having in consideration that we have labeled data, in a time series format. To train it, there are several options - some of the common ones are neural networks such as LSTM. We will start by scaling the features:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaled_np = scaler.fit_transform(rare_ds[common_features])\nscaled_df = pd.DataFrame(scaled_np, columns=common_features)\nscaled_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:57.403497Z","iopub.execute_input":"2021-11-27T08:53:57.404294Z","iopub.status.idle":"2021-11-27T08:53:57.446397Z","shell.execute_reply.started":"2021-11-27T08:53:57.404251Z","shell.execute_reply":"2021-11-27T08:53:57.445645Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# add the time and anomaly\nfinal_df = pd.concat([rare_ds[['time', 'anomaly']], scaled_df], axis=1)\nfinal_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:57.447827Z","iopub.execute_input":"2021-11-27T08:53:57.448268Z","iopub.status.idle":"2021-11-27T08:53:57.475729Z","shell.execute_reply.started":"2021-11-27T08:53:57.448231Z","shell.execute_reply":"2021-11-27T08:53:57.475056Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Next thing will be to convert the timestamp to datetime, and then we need to prepare data to have regular intervals. We check the interval time, and we can see that we have information for over 15 hours, for one day. We can see that the data is already divided into regular intervals of one second.","metadata":{}},{"cell_type":"code","source":"final_df['time'] = pd.to_datetime(final_df['time'], unit='s')\nprint(final_df[\"time\"].head())\nprint(min(final_df[\"time\"]))\nprint(max(final_df[\"time\"]))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:57.476753Z","iopub.execute_input":"2021-11-27T08:53:57.477281Z","iopub.status.idle":"2021-11-27T08:53:57.511767Z","shell.execute_reply.started":"2021-11-27T08:53:57.477242Z","shell.execute_reply":"2021-11-27T08:53:57.511105Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The next step is to tranform this data into a time series problem. In orer to do that, first we need to decide the time window. This is where the data we dropped before - with Instance information - can be useful, because it shows the time where the failure was introduced, and the time it took to fail. Let's plot it: ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11, 4)})\noriginal_rare_ds.columns =[col.strip() for col in original_rare_ds.columns]\noriginal_rare_ds['time'] = pd.to_datetime(original_rare_ds['time'], unit='s')\noriginal_df = original_rare_ds.set_index(\"time\")\noriginal_df['factor'] = pd.factorize(original_df[\"instance\"])[0]\noriginal_df['factor'].plot(linewidth=0.5);","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:57.512956Z","iopub.execute_input":"2021-11-27T08:53:57.513326Z","iopub.status.idle":"2021-11-27T08:53:58.474526Z","shell.execute_reply.started":"2021-11-27T08:53:57.513293Z","shell.execute_reply":"2021-11-27T08:53:58.473833Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"original_df[['factor', 'anomaly']].loc['2020-01-14 20:17:00':'2020-01-14 21:17:00'].plot(linewidth=0.5);","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:58.475921Z","iopub.execute_input":"2021-11-27T08:53:58.476186Z","iopub.status.idle":"2021-11-27T08:53:59.445323Z","shell.execute_reply.started":"2021-11-27T08:53:58.476150Z","shell.execute_reply":"2021-11-27T08:53:59.444595Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"original_df[['factor', 'anomaly']].loc['2020-01-14 20:24:00':'2020-01-14 20:32:00'].plot(linewidth=0.5);","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:59.446451Z","iopub.execute_input":"2021-11-27T08:53:59.447050Z","iopub.status.idle":"2021-11-27T08:53:59.778510Z","shell.execute_reply.started":"2021-11-27T08:53:59.446990Z","shell.execute_reply":"2021-11-27T08:53:59.777840Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"We can see that the tests have been executed in time windows of 7:30 minutes. The lag will be over 1:40. So we will take that as our window for LSTM. Let's create the function first:","metadata":{}},{"cell_type":"code","source":"def series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Target timestep (t=lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:59.779902Z","iopub.execute_input":"2021-11-27T08:53:59.780412Z","iopub.status.idle":"2021-11-27T08:53:59.789218Z","shell.execute_reply.started":"2021-11-27T08:53:59.780372Z","shell.execute_reply":"2021-11-27T08:53:59.788334Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# as data is in seconds, the window will be calculated in this unit\nwindow = int(7.5*60.0)\nlag = 100\nseries = series_to_supervised(final_df.drop('time', axis=1), window=window, lag=lag)\nseries.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:53:59.793476Z","iopub.execute_input":"2021-11-27T08:53:59.793696Z","iopub.status.idle":"2021-11-27T08:54:01.940632Z","shell.execute_reply.started":"2021-11-27T08:53:59.793663Z","shell.execute_reply":"2021-11-27T08:54:01.939872Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"* Once we have the data as a time serie, is time to divide into train and test. In this case, we are going to use the first measures in time as train and the last measures as test. So we cannot shuffle our data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Label\nlabels_col = 'anomaly(t+%d)' % lag\nlabels = series[labels_col]\nseries = series.drop(labels_col, axis=1)\n\nX_total, X_test, Y_total, Y_test = train_test_split(series, labels.values, test_size=0.2, random_state=0, shuffle=False)\nprint(X_test.shape)\nprint(Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:54:01.941901Z","iopub.execute_input":"2021-11-27T08:54:01.942355Z","iopub.status.idle":"2021-11-27T08:54:02.450631Z","shell.execute_reply.started":"2021-11-27T08:54:01.942314Z","shell.execute_reply":"2021-11-27T08:54:02.449877Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.2, random_state=0, shuffle=False)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_valid.shape)\nprint(Y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:54:02.451905Z","iopub.execute_input":"2021-11-27T08:54:02.452184Z","iopub.status.idle":"2021-11-27T08:54:02.714444Z","shell.execute_reply.started":"2021-11-27T08:54:02.452145Z","shell.execute_reply":"2021-11-27T08:54:02.713657Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Once we have the data in the proper shape, and it is divided properly into train/validation/test, is time to start applying a model. We are going to use an LSTM model, that will be able to pick information from a sequence and will be able to learn patterns, specially if they are long sequences. ","metadata":{}},{"cell_type":"code","source":"# prepare to apply LSTM\nX_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\nprint('Train set shape', X_train_series.shape)\nprint('Validation set shape', X_valid_series.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:54:02.715729Z","iopub.execute_input":"2021-11-27T08:54:02.716174Z","iopub.status.idle":"2021-11-27T08:54:02.971458Z","shell.execute_reply.started":"2021-11-27T08:54:02.716134Z","shell.execute_reply":"2021-11-27T08:54:02.970622Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearlyStop=EarlyStopping(monitor=\"val_loss\",verbose=0,mode='min',patience=3)\n\nepochs = 10\nbatch = 256\nn_neurons=10","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:54:02.972572Z","iopub.execute_input":"2021-11-27T08:54:02.973323Z","iopub.status.idle":"2021-11-27T08:54:02.991194Z","shell.execute_reply.started":"2021-11-27T08:54:02.973280Z","shell.execute_reply":"2021-11-27T08:54:02.990450Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(units=n_neurons, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\nmodel_lstm.add(Dropout(0.2))\nmodel_lstm.add(Dense(1, activation='sigmoid'))\nmodel_lstm.compile(loss='binary_crossentropy', optimizer='adam', validation_data=(X_valid_series, Y_valid), metrics=['accuracy'])\nmodel_lstm.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:54:02.992673Z","iopub.execute_input":"2021-11-27T08:54:02.993181Z","iopub.status.idle":"2021-11-27T08:54:03.168875Z","shell.execute_reply.started":"2021-11-27T08:54:02.993144Z","shell.execute_reply":"2021-11-27T08:54:03.168046Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"lstm_history = model_lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=epochs, batch_size=batch, verbose=1, callbacks=[earlyStop])","metadata":{"execution":{"iopub.status.busy":"2021-11-27T08:54:03.170323Z","iopub.execute_input":"2021-11-27T08:54:03.170595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}