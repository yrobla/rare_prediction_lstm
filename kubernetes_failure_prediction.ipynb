{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-15T14:22:12.202240Z","iopub.execute_input":"2021-12-15T14:22:12.203097Z","iopub.status.idle":"2021-12-15T14:22:12.240758Z","shell.execute_reply.started":"2021-12-15T14:22:12.202975Z","shell.execute_reply":"2021-12-15T14:22:12.239804Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# initialize seed, so we get reproducible results\nfrom numpy.random import seed\nimport tensorflow as tf\nimport os\nimport random\nfrom tensorflow.python.eager import context\n\nSEED_VALUE = 0\nos.environ[\"PYTHONHASHSEED\"]=str(SEED_VALUE)\nrandom.seed(SEED_VALUE)\n\nseed(SEED_VALUE)\ntf.random.set_seed(SEED_VALUE)\n\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:12.268584Z","iopub.execute_input":"2021-12-15T14:22:12.269599Z","iopub.status.idle":"2021-12-15T14:22:18.821851Z","shell.execute_reply.started":"2021-12-15T14:22:12.269485Z","shell.execute_reply":"2021-12-15T14:22:18.820971Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# import the rare dataset\nrare_ds = pd.read_csv(\"/kaggle/input/rare-kubernetes-anomalies/RARE.csv\", sep=\"|\", header=0, low_memory=False)\nrare_ds.head()\noriginal_rare_ds = rare_ds.copy()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:18.823893Z","iopub.execute_input":"2021-12-15T14:22:18.824175Z","iopub.status.idle":"2021-12-15T14:22:37.509073Z","shell.execute_reply.started":"2021-12-15T14:22:18.824145Z","shell.execute_reply":"2021-12-15T14:22:37.508084Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(rare_ds.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:37.510537Z","iopub.execute_input":"2021-12-15T14:22:37.510850Z","iopub.status.idle":"2021-12-15T14:22:37.516423Z","shell.execute_reply.started":"2021-12-15T14:22:37.510811Z","shell.execute_reply":"2021-12-15T14:22:37.515476Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"rare_ds.dtypes.unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:37.517744Z","iopub.execute_input":"2021-12-15T14:22:37.518053Z","iopub.status.idle":"2021-12-15T14:22:37.536263Z","shell.execute_reply.started":"2021-12-15T14:22:37.518011Z","shell.execute_reply":"2021-12-15T14:22:37.535405Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We can observe that we have a time series dataset, which contains the timestamp as first column, and anomaly as second column. Thee is the isntance field, that contains a descriptive value of the cluster status on that moment.\nThen, there are a set of prometheus metrics, collected for the specific timestamp. The anomaly is the field that we are going to use for training, and the other entries are numerical entries, that are going to influence the model.\nWe have a total of 10.010 rows for training, and a total of 7063 features, that will need to be selected.\nLet's show the different values for the anomaly and instance field:","metadata":{}},{"cell_type":"code","source":"rare_ds.columns =[col.strip() for col in rare_ds.columns]\nprint(rare_ds[\"anomaly\"].unique())\nprint(rare_ds[\"instance\"].unique())","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:37.539124Z","iopub.execute_input":"2021-12-15T14:22:37.539812Z","iopub.status.idle":"2021-12-15T14:22:37.556992Z","shell.execute_reply.started":"2021-12-15T14:22:37.539767Z","shell.execute_reply":"2021-12-15T14:22:37.555813Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We can see that the anomaly is just a binary field, that we can use to know if there is an anomaly or not. We can check the distribution of the field. ","metadata":{}},{"cell_type":"code","source":"rare_ds.hist(column='anomaly')","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:37.558828Z","iopub.execute_input":"2021-12-15T14:22:37.559691Z","iopub.status.idle":"2021-12-15T14:22:37.794689Z","shell.execute_reply.started":"2021-12-15T14:22:37.559656Z","shell.execute_reply":"2021-12-15T14:22:37.793771Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We can see that the anomaly is just a binary field, that we can use to know if there is an anomaly or not. For our case, we can drop the instance field, as it is not relevant for training based on anomalies.","metadata":{}},{"cell_type":"code","source":"rare_ds.drop(labels=['instance'], axis=1, inplace = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:37.796467Z","iopub.execute_input":"2021-12-15T14:22:37.796798Z","iopub.status.idle":"2021-12-15T14:22:37.990850Z","shell.execute_reply.started":"2021-12-15T14:22:37.796738Z","shell.execute_reply":"2021-12-15T14:22:37.990114Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":" Now we need to proceed with feature selection. We have a total of 7060 metrics and the objective is to select the relevant ones for the project. In order to do it, we could proceed with manual removal first, using the Kubernetes domain knowledge, to discard the non meaningful ones:\n - any information about resource creation is not relevant, because it depends on the workloads that are existing on each cluster\n - kafka topics, logs... are a consequence of other operations in the cluster, they cannot be part of cluster failures reason\n - any information related about pod status, container status.. that have some fixed id needs to be discarded, as it is something that will change depending on the workloads and cluster","metadata":{}},{"cell_type":"code","source":"final_columns = []\nfor column in sorted(rare_ds.columns):\n    if not column.strip().startswith((\"kafka_topic\", \"kafka_log\", \"kube_configmap\", \"kube_namespace\", \"kube_service\", \"kube_secret\", \"kube_pod_status\", \"kube_pod_container_status\", \"scrape_samples\", \"kafka_server_brokertopic\")):\n        final_columns.append(column.strip())","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:37.993734Z","iopub.execute_input":"2021-12-15T14:22:37.993976Z","iopub.status.idle":"2021-12-15T14:22:38.005864Z","shell.execute_reply.started":"2021-12-15T14:22:37.993947Z","shell.execute_reply":"2021-12-15T14:22:38.005061Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"features_ds = rare_ds[final_columns].copy()\nfeatures_ds.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:38.007458Z","iopub.execute_input":"2021-12-15T14:22:38.007777Z","iopub.status.idle":"2021-12-15T14:22:38.269664Z","shell.execute_reply.started":"2021-12-15T14:22:38.007734Z","shell.execute_reply":"2021-12-15T14:22:38.269065Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"After manual selection we will use automated methods. First step would be to remove the constant features. Any metric that is having a constant value, with no relevant variance over the dataset, is a metric that is not useful to discriminate a target. We can find those constant values using variance:","metadata":{}},{"cell_type":"code","source":"features_ds.dtypes.unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:38.270699Z","iopub.execute_input":"2021-12-15T14:22:38.271073Z","iopub.status.idle":"2021-12-15T14:22:38.277324Z","shell.execute_reply.started":"2021-12-15T14:22:38.271045Z","shell.execute_reply":"2021-12-15T14:22:38.276488Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# for all columns that are objects, convert to float\nfor column in features_ds.columns:\n    if features_ds[column].dtype == \"object\":\n        features_ds[column] = features_ds[column].astype(float)\nfeatures_ds.dtypes.unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:38.278409Z","iopub.execute_input":"2021-12-15T14:22:38.278615Z","iopub.status.idle":"2021-12-15T14:22:38.801643Z","shell.execute_reply.started":"2021-12-15T14:22:38.278591Z","shell.execute_reply":"2021-12-15T14:22:38.800812Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=0.1)\nsel.fit(features_ds)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:38.802757Z","iopub.execute_input":"2021-12-15T14:22:38.803059Z","iopub.status.idle":"2021-12-15T14:22:40.679762Z","shell.execute_reply.started":"2021-12-15T14:22:38.803025Z","shell.execute_reply":"2021-12-15T14:22:40.679081Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# get a list of the constant features. Those can be removed\nprint(\n    len([\n        x for x in features_ds.columns\n        if x not in features_ds.columns[sel.get_support()]\n    ]))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:40.681035Z","iopub.execute_input":"2021-12-15T14:22:40.681746Z","iopub.status.idle":"2021-12-15T14:22:41.267668Z","shell.execute_reply.started":"2021-12-15T14:22:40.681708Z","shell.execute_reply":"2021-12-15T14:22:41.266807Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"to_strip = [x for x in features_ds.columns if x not in features_ds.columns[sel.get_support()]]\nfeatures_ds_strip = features_ds.drop(labels=to_strip, axis=1)\nfeatures_ds_strip.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:41.270926Z","iopub.execute_input":"2021-12-15T14:22:41.271213Z","iopub.status.idle":"2021-12-15T14:22:41.896256Z","shell.execute_reply.started":"2021-12-15T14:22:41.271182Z","shell.execute_reply":"2021-12-15T14:22:41.895509Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"With that technique we have reduced the number of features to 1740. It is still a long number of features, so we need to apply more feature reduction techniques to select the most relevant ones. As we deal with a large amount of features, we will reduce them using Mutual Information technique:","metadata":{}},{"cell_type":"code","source":"X = features_ds_strip\nY = rare_ds[\"anomaly\"].to_frame()\nprint(X.shape)\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:41.897518Z","iopub.execute_input":"2021-12-15T14:22:41.897738Z","iopub.status.idle":"2021-12-15T14:22:41.903544Z","shell.execute_reply.started":"2021-12-15T14:22:41.897711Z","shell.execute_reply":"2021-12-15T14:22:41.902735Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# select with mutual information\nfrom sklearn.feature_selection import SelectPercentile as SP\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.tree import DecisionTreeClassifier as DTC\n\nselector = SP(percentile=2) # select features with top 2% MI scores\nselector.fit(X,Y.values.ravel())\nX_4 = selector.transform(X)\nX_train_4,X_test_4,y_train,y_test = tts(\n    X_4,Y\n    ,random_state=0\n    ,stratify=Y\n)\nmodel_4 = DTC().fit(X_train_4,y_train)\nscore_4 = model_4.score(X_test_4,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:41.904750Z","iopub.execute_input":"2021-12-15T14:22:41.905270Z","iopub.status.idle":"2021-12-15T14:22:42.517288Z","shell.execute_reply.started":"2021-12-15T14:22:41.905236Z","shell.execute_reply":"2021-12-15T14:22:42.516249Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(f\"score_4:{score_4}\")\nprint(X_4.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:42.518442Z","iopub.execute_input":"2021-12-15T14:22:42.518662Z","iopub.status.idle":"2021-12-15T14:22:42.523271Z","shell.execute_reply.started":"2021-12-15T14:22:42.518636Z","shell.execute_reply":"2021-12-15T14:22:42.522426Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# get columns, we have reduced to a total of 35\ncolumns = np.asarray(X.columns.values)\nsupport = np.asarray(selector.get_support())\ncolumns_with_support = columns[support]\nprint(columns_with_support)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:42.524537Z","iopub.execute_input":"2021-12-15T14:22:42.524830Z","iopub.status.idle":"2021-12-15T14:22:42.535678Z","shell.execute_reply.started":"2021-12-15T14:22:42.524794Z","shell.execute_reply":"2021-12-15T14:22:42.534775Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Using the Mutual Information method, for the 3% with bigger MI score, returned a total of 36 columns. With a more reasonable number of features, we can now use visualization to discard the highly correlated ones.","metadata":{}},{"cell_type":"code","source":"# with this reduced amount of features, now we can do a correlation map\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\nx = rare_ds[columns_with_support]\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:42.537096Z","iopub.execute_input":"2021-12-15T14:22:42.537961Z","iopub.status.idle":"2021-12-15T14:22:50.101942Z","shell.execute_reply.started":"2021-12-15T14:22:42.537914Z","shell.execute_reply":"2021-12-15T14:22:50.101125Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# retrieve the top correlated values to remove them. We are going to list the pairs with more correlation\n# and remove vars over that threshold, so we reduce the redundant features\nc = x.corr().abs().unstack().sort_values().drop_duplicates()\nc = c[c>0.9]\n\nto_remove = [\"java_lang_garbagecollector_committed_lastgcinfo_memoryusageaftergc_g1_young_generation_key_g1_eden_space_0\", \n             \"java_lang_garbagecollector_committed_lastgcinfo_memoryusagebeforegc_g1_young_generation_key_g1_old_gen_0\",\n             \"java_lang_memorypool_committed_collectionusage_g1_old_gen_0\", \"node_memory_Cached_bytes_0\",\n            \"node_memory_Inactive_file_bytes_0\", \"node_memory_AnonPages_bytes_0\", \"node_memory_Active_anon_bytes_0\",\n             \"node_memory_Slab_bytes_0\", \"java_lang_operatingsystem_systemloadaverage_0\", \"node_memory_Inactive_bytes_0\",\n             \"node_memory_Active_file_bytes_0\", \"node_memory_SUnreclaim_bytes_0\", \"java_lang_memorypool_committed_usage_g1_old_gen_0\",\n             \"java_lang_garbagecollector_committed_lastgcinfo_memoryusageaftergc_g1_young_generation_key_g1_old_gen_0\"\n             ]\n    \ncolumns_to_remove = list(set(to_remove))\ncommon_features = set(x.columns) - set(columns_to_remove)\nprint(common_features)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:50.103823Z","iopub.execute_input":"2021-12-15T14:22:50.104333Z","iopub.status.idle":"2021-12-15T14:22:50.159564Z","shell.execute_reply.started":"2021-12-15T14:22:50.104300Z","shell.execute_reply":"2021-12-15T14:22:50.158695Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Once the common features have been selected, let's do some basic analysis on them, and will plot them to show their trend and seasonality. We can see that each var has independent distribution.","metadata":{}},{"cell_type":"code","source":"rare_ds[common_features].describe().apply(lambda s: s.apply('{0:.5f}'.format))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:50.161317Z","iopub.execute_input":"2021-12-15T14:22:50.161805Z","iopub.status.idle":"2021-12-15T14:22:50.252279Z","shell.execute_reply.started":"2021-12-15T14:22:50.161775Z","shell.execute_reply":"2021-12-15T14:22:50.251229Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"rare_ds[common_features].plot(subplots=True, layout=(4,6), figsize=(25,25))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:50.254292Z","iopub.execute_input":"2021-12-15T14:22:50.254723Z","iopub.status.idle":"2021-12-15T14:22:56.654746Z","shell.execute_reply.started":"2021-12-15T14:22:50.254692Z","shell.execute_reply":"2021-12-15T14:22:56.654004Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"The data inspection shows that the variables have different value ranges, so we will need to scale them in order to apply training models.","metadata":{"execution":{"iopub.status.busy":"2021-12-07T11:02:48.862555Z","iopub.execute_input":"2021-12-07T11:02:48.862789Z","iopub.status.idle":"2021-12-07T11:02:48.881809Z","shell.execute_reply.started":"2021-12-07T11:02:48.862759Z","shell.execute_reply":"2021-12-07T11:02:48.879509Z"}}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfinal_df = rare_ds[common_features]\nfinal_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:56.656571Z","iopub.execute_input":"2021-12-15T14:22:56.657071Z","iopub.status.idle":"2021-12-15T14:22:56.686430Z","shell.execute_reply.started":"2021-12-15T14:22:56.657006Z","shell.execute_reply":"2021-12-15T14:22:56.685312Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# add the anomaly column to the final dataframe to compute it\nfinal_df = pd.concat([rare_ds[['anomaly']], final_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:56.687781Z","iopub.execute_input":"2021-12-15T14:22:56.688026Z","iopub.status.idle":"2021-12-15T14:22:56.701054Z","shell.execute_reply.started":"2021-12-15T14:22:56.687966Z","shell.execute_reply":"2021-12-15T14:22:56.700275Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Next thing will be to convert the timestamp to datetime, and then we need to prepare data to have regular intervals. We check the interval time, and we can see that we have information for over 15 hours, for one day. We can see that the data is already divided into regular intervals of one second. The next step is to tranform this data into a time series problem. In orer to do that, first we need to decide the time window. This is where the data we dropped before - with Instance information - can be useful, because it shows the time where the failure was introduced, and the time it took to fail. Let's plot it: ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11, 4)})\noriginal_rare_ds.columns =[col.strip() for col in original_rare_ds.columns]\noriginal_df = original_rare_ds.set_index(\"time\")\noriginal_df['anomaly'].plot(linewidth=0.5);","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:56.702483Z","iopub.execute_input":"2021-12-15T14:22:56.703309Z","iopub.status.idle":"2021-12-15T14:22:57.278587Z","shell.execute_reply.started":"2021-12-15T14:22:56.703259Z","shell.execute_reply":"2021-12-15T14:22:57.277559Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"original_df[['anomaly']].iloc[0:1000].plot(linewidth=0.5);","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:57.280347Z","iopub.execute_input":"2021-12-15T14:22:57.280670Z","iopub.status.idle":"2021-12-15T14:22:57.983444Z","shell.execute_reply.started":"2021-12-15T14:22:57.280625Z","shell.execute_reply":"2021-12-15T14:22:57.982500Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"original_df[['anomaly']].iloc[3000:3500].plot(linewidth=0.5);","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:57.984886Z","iopub.execute_input":"2021-12-15T14:22:57.985205Z","iopub.status.idle":"2021-12-15T14:22:58.243531Z","shell.execute_reply.started":"2021-12-15T14:22:57.985165Z","shell.execute_reply":"2021-12-15T14:22:58.242682Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"We can see that the tests have been executed regularly in intervals of over 400 iterations. The anomaly is present in an interval around 25 iterations:","metadata":{"execution":{"iopub.status.busy":"2021-12-07T11:04:54.899928Z","iopub.execute_input":"2021-12-07T11:04:54.900169Z","iopub.status.idle":"2021-12-07T11:04:54.907871Z","shell.execute_reply.started":"2021-12-07T11:04:54.90014Z","shell.execute_reply":"2021-12-07T11:04:54.905807Z"}}},{"cell_type":"code","source":"# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:58.245057Z","iopub.execute_input":"2021-12-15T14:22:58.245514Z","iopub.status.idle":"2021-12-15T14:22:58.255991Z","shell.execute_reply.started":"2021-12-15T14:22:58.245468Z","shell.execute_reply":"2021-12-15T14:22:58.255141Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\n# integer encode direction\nencoder = LabelEncoder()\nvalues=final_df.values\nvalues[:,1] = encoder.fit_transform(values[:,1])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:58.257204Z","iopub.execute_input":"2021-12-15T14:22:58.257515Z","iopub.status.idle":"2021-12-15T14:22:58.270358Z","shell.execute_reply.started":"2021-12-15T14:22:58.257484Z","shell.execute_reply":"2021-12-15T14:22:58.269552Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# ensure all data is float\nvalues = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:58.271571Z","iopub.execute_input":"2021-12-15T14:22:58.272025Z","iopub.status.idle":"2021-12-15T14:22:58.281105Z","shell.execute_reply.started":"2021-12-15T14:22:58.271969Z","shell.execute_reply":"2021-12-15T14:22:58.280280Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"PERIOD=300\n\n# first drop the features for last period\nreframed = series_to_supervised(scaled, PERIOD, PERIOD)\nreframed.drop(reframed.columns[list(range(len(reframed.columns)-len(common_features),len(reframed.columns)))], axis=1, inplace=True)\n\n# and also drop the var1 (variable to be predicted, but first extracting the last one)\ntarget_var = reframed.iloc[:,-1:]\nreframed = reframed[reframed.columns.drop(list(reframed.filter(regex='var1')))]\n\nreframed.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:22:58.282660Z","iopub.execute_input":"2021-12-15T14:22:58.282963Z","iopub.status.idle":"2021-12-15T14:23:01.868069Z","shell.execute_reply.started":"2021-12-15T14:22:58.282924Z","shell.execute_reply":"2021-12-15T14:23:01.867083Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"target_var.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:23:01.869838Z","iopub.execute_input":"2021-12-15T14:23:01.870167Z","iopub.status.idle":"2021-12-15T14:23:01.880560Z","shell.execute_reply.started":"2021-12-15T14:23:01.870123Z","shell.execute_reply":"2021-12-15T14:23:01.879682Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(reframed.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:23:01.882227Z","iopub.execute_input":"2021-12-15T14:23:01.882551Z","iopub.status.idle":"2021-12-15T14:23:01.891399Z","shell.execute_reply.started":"2021-12-15T14:23:01.882508Z","shell.execute_reply":"2021-12-15T14:23:01.890678Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# split into train and test sets\nvalues = reframed.values\ntarget_values = target_var.values.astype(int)\n\ntrain_time=int(values.shape[0]*0.8)\ntest_time=int(values.shape[0]*0.9)\n\ntrain_X = values[:train_time, :]\nvalid_X = values[train_time:test_time, :]\ntest_X = values[test_time:, :]\n\ntrain_y = target_values[:train_time, :]\nvalid_y = target_values[train_time:test_time, :]\ntest_y = target_values[test_time:, :]\n\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\nvalid_X = valid_X.reshape((valid_X.shape[0], 1, valid_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:23:01.892587Z","iopub.execute_input":"2021-12-15T14:23:01.893083Z","iopub.status.idle":"2021-12-15T14:23:01.904784Z","shell.execute_reply.started":"2021-12-15T14:23:01.893051Z","shell.execute_reply":"2021-12-15T14:23:01.903902Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"To train our model, we are interested in the F1 score, not accuracy. The reason is that we want to predict properly positive and negative classes, and as the dataset is imbalanced, accuracy will be a false metric, because it will give high results even if it fails in the positive class. So, we can create a custom f1 score metric, and train based on it.","metadata":{}},{"cell_type":"code","source":"import keras.backend as K\n\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:23:01.905988Z","iopub.execute_input":"2021-12-15T14:23:01.906342Z","iopub.status.idle":"2021-12-15T14:23:01.914049Z","shell.execute_reply.started":"2021-12-15T14:23:01.906312Z","shell.execute_reply":"2021-12-15T14:23:01.913178Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Once we have the data in the proper shape, and it is divided properly into train/validation/test, is time to start applying a model. We are going to use an LSTM model, that will be able to pick information from a sequence and will be able to learn patterns, specially if they are long sequences. ","metadata":{}},{"cell_type":"code","source":"### design network\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.metrics import AUC\n\nmodel1 = Sequential()\nmodel1.add(LSTM(units=50, return_sequences=False, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel1.add(Dense(units=10, activation='relu'))\nmodel1.add(Dense(units=1, activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC(name='auc'), AUC(name='prc', curve='PR')])\nmodel1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:23:01.915583Z","iopub.execute_input":"2021-12-15T14:23:01.916438Z","iopub.status.idle":"2021-12-15T14:23:02.216439Z","shell.execute_reply.started":"2021-12-15T14:23:01.916404Z","shell.execute_reply":"2021-12-15T14:23:02.215554Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# fit network\nfrom keras.callbacks import EarlyStopping\n\nEPOCHS=100\nes = EarlyStopping(monitor=get_f1, mode='max', verbose=0, patience=10, restore_best_weights=True)\n\nhistory1 = model1.fit(train_X, train_y, epochs=EPOCHS, batch_size=60, validation_data=(valid_X, valid_y), verbose=0, shuffle=False, callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:23:02.217630Z","iopub.execute_input":"2021-12-15T14:23:02.217863Z","iopub.status.idle":"2021-12-15T14:26:26.854821Z","shell.execute_reply.started":"2021-12-15T14:23:02.217833Z","shell.execute_reply":"2021-12-15T14:26:26.853664Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history1.history['loss'], label='train')\npyplot.plot(history1.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:26:26.856853Z","iopub.execute_input":"2021-12-15T14:26:26.857144Z","iopub.status.idle":"2021-12-15T14:26:27.091296Z","shell.execute_reply.started":"2021-12-15T14:26:26.857110Z","shell.execute_reply":"2021-12-15T14:26:27.090438Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history1.history['prc'], label='train')\npyplot.plot(history1.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:26:27.092655Z","iopub.execute_input":"2021-12-15T14:26:27.092895Z","iopub.status.idle":"2021-12-15T14:26:27.310210Z","shell.execute_reply.started":"2021-12-15T14:26:27.092865Z","shell.execute_reply":"2021-12-15T14:26:27.309378Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndef evaluate_and_predict(model, test_X, test_y):\n    scores = model.evaluate(test_X, test_y, verbose=0)\n    print(\"AUC ROC: {}\".format(scores[1]))\n    print(\"AUC precision-recall: {}\".format(scores[2]))  \n    \n    y_pred = model.predict(test_X)\n    predict_class = ((y_pred > 0.5)+0).ravel()\n    print(confusion_matrix(test_y, predict_class))\n    print(\"F1 score for 50%: {}\".format(f1_score(test_y, predict_class)))\n    \n    predict_class = ((y_pred > 0.2)+0).ravel()\n    print(confusion_matrix(test_y, predict_class))\n    print(\"F1 score for 20%: {}\".format(f1_score(test_y, predict_class)))","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:26:27.311658Z","iopub.execute_input":"2021-12-15T14:26:27.311891Z","iopub.status.idle":"2021-12-15T14:26:27.319847Z","shell.execute_reply.started":"2021-12-15T14:26:27.311862Z","shell.execute_reply":"2021-12-15T14:26:27.319054Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"We have plotted the chart to see the train and validation loss, and PR curve. We can see that model is finally converging.","metadata":{}},{"cell_type":"markdown","source":"Now let's predict values with the testing data we separated from our dataset.","metadata":{}},{"cell_type":"code","source":"evaluate_and_predict(model1, test_X, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:26:27.325445Z","iopub.execute_input":"2021-12-15T14:26:27.325669Z","iopub.status.idle":"2021-12-15T14:26:27.949554Z","shell.execute_reply.started":"2021-12-15T14:26:27.325644Z","shell.execute_reply":"2021-12-15T14:26:27.948697Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Although the ROC and PR metrics are quite good, the F1 score and the ability to predict classes correctly - specially the positive class - is still not accurate. So we will try different options.","metadata":{}},{"cell_type":"code","source":"history2 = model1.fit(train_X, train_y, epochs=EPOCHS, batch_size=PERIOD, validation_data=(valid_X, valid_y), verbose=0, shuffle=False, callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:26:27.950880Z","iopub.execute_input":"2021-12-15T14:26:27.951211Z","iopub.status.idle":"2021-12-15T14:27:43.087472Z","shell.execute_reply.started":"2021-12-15T14:26:27.951169Z","shell.execute_reply":"2021-12-15T14:27:43.086398Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history2.history['loss'], label='train')\npyplot.plot(history2.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:27:43.088909Z","iopub.execute_input":"2021-12-15T14:27:43.089477Z","iopub.status.idle":"2021-12-15T14:27:43.336432Z","shell.execute_reply.started":"2021-12-15T14:27:43.089444Z","shell.execute_reply":"2021-12-15T14:27:43.335401Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history2.history['prc'], label='train')\npyplot.plot(history2.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:27:43.337613Z","iopub.execute_input":"2021-12-15T14:27:43.337859Z","iopub.status.idle":"2021-12-15T14:27:43.561664Z","shell.execute_reply.started":"2021-12-15T14:27:43.337829Z","shell.execute_reply":"2021-12-15T14:27:43.560886Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"evaluate_and_predict(model1, test_X, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:27:43.563244Z","iopub.execute_input":"2021-12-15T14:27:43.563700Z","iopub.status.idle":"2021-12-15T14:27:43.886939Z","shell.execute_reply.started":"2021-12-15T14:27:43.563657Z","shell.execute_reply":"2021-12-15T14:27:43.886011Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model3 = Sequential()\nmodel3.add(LSTM(units=256, return_sequences=False, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel3.add(Dense(units=10, activation='relu'))\nmodel3.add(Dense(units=1, activation='sigmoid'))\nmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC(name='auc'), AUC(name='prc', curve='PR')])\nmodel3.summary()\nhistory3 = model3.fit(train_X, train_y, epochs=EPOCHS, batch_size=PERIOD, validation_data=(valid_X, valid_y), verbose=0, shuffle=False, callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:27:43.888075Z","iopub.execute_input":"2021-12-15T14:27:43.888294Z","iopub.status.idle":"2021-12-15T14:33:10.280808Z","shell.execute_reply.started":"2021-12-15T14:27:43.888267Z","shell.execute_reply":"2021-12-15T14:33:10.280087Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history3.history['loss'], label='train')\npyplot.plot(history3.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:33:10.282899Z","iopub.execute_input":"2021-12-15T14:33:10.283258Z","iopub.status.idle":"2021-12-15T14:33:11.202843Z","shell.execute_reply.started":"2021-12-15T14:33:10.283217Z","shell.execute_reply":"2021-12-15T14:33:11.201957Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history3.history['prc'], label='train')\npyplot.plot(history3.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:33:11.204214Z","iopub.execute_input":"2021-12-15T14:33:11.204463Z","iopub.status.idle":"2021-12-15T14:33:11.430355Z","shell.execute_reply.started":"2021-12-15T14:33:11.204432Z","shell.execute_reply":"2021-12-15T14:33:11.429569Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"We can see good results, where validation loss is finally converging, with some regular intervals. Let's evaluate the model to decide which one is best:","metadata":{"execution":{"iopub.status.busy":"2021-12-07T11:16:58.714523Z","iopub.execute_input":"2021-12-07T11:16:58.714836Z","iopub.status.idle":"2021-12-07T11:16:58.721887Z","shell.execute_reply.started":"2021-12-07T11:16:58.714795Z","shell.execute_reply":"2021-12-07T11:16:58.72048Z"}}},{"cell_type":"code","source":"evaluate_and_predict(model3, test_X, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:33:11.432011Z","iopub.execute_input":"2021-12-15T14:33:11.432307Z","iopub.status.idle":"2021-12-15T14:33:12.352550Z","shell.execute_reply.started":"2021-12-15T14:33:11.432268Z","shell.execute_reply":"2021-12-15T14:33:12.351687Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"model4 = Sequential()\nmodel4.add(LSTM(units=512, return_sequences=False, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel4.add(Dense(units=256, activation='relu'))\nmodel4.add(Dropout(0.2, seed=SEED_VALUE))\nmodel4.add(Dense(units=1, activation='sigmoid'))\nmodel4.compile(loss='binary_crossentropy', optimizer='adam', metrics=[AUC(name='auc'), AUC(name='prc', curve='PR')])\nmodel4.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:33:12.353710Z","iopub.execute_input":"2021-12-15T14:33:12.353946Z","iopub.status.idle":"2021-12-15T14:33:12.744040Z","shell.execute_reply.started":"2021-12-15T14:33:12.353918Z","shell.execute_reply":"2021-12-15T14:33:12.743370Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"history4 = model4.fit(train_X, train_y, epochs=EPOCHS, batch_size=PERIOD, validation_data=(valid_X, valid_y), verbose=0, shuffle=False, callbacks=[es])","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:33:12.745085Z","iopub.execute_input":"2021-12-15T14:33:12.745825Z","iopub.status.idle":"2021-12-15T14:44:17.180159Z","shell.execute_reply.started":"2021-12-15T14:33:12.745775Z","shell.execute_reply":"2021-12-15T14:44:17.179437Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history4.history['loss'], label='train')\npyplot.plot(history4.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:44:17.181958Z","iopub.execute_input":"2021-12-15T14:44:17.182446Z","iopub.status.idle":"2021-12-15T14:44:17.388677Z","shell.execute_reply.started":"2021-12-15T14:44:17.182401Z","shell.execute_reply":"2021-12-15T14:44:17.387867Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(history4.history['prc'], label='train')\npyplot.plot(history4.history['val_prc'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:44:17.389911Z","iopub.execute_input":"2021-12-15T14:44:17.390161Z","iopub.status.idle":"2021-12-15T14:44:17.653631Z","shell.execute_reply.started":"2021-12-15T14:44:17.390132Z","shell.execute_reply":"2021-12-15T14:44:17.652828Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"evaluate_and_predict(model4, test_X, test_y)","metadata":{"execution":{"iopub.status.busy":"2021-12-15T14:44:17.654914Z","iopub.execute_input":"2021-12-15T14:44:17.655824Z","iopub.status.idle":"2021-12-15T14:44:19.035747Z","shell.execute_reply.started":"2021-12-15T14:44:17.655777Z","shell.execute_reply":"2021-12-15T14:44:19.034869Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model has wors precision/recall than previews, and also worse F1 score, but is predicting positive classes in a more accurate way. So if our intention is to predict positive classes, we should pick this one.","metadata":{}}]}